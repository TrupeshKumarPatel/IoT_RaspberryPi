@misc{ibm100deepblue, 
	title={Deep Blue},
	author = {Chung-Jen Tan and Murray Campbell and Feng-hsiung Hsu and Joseph Hoane Jr. and Jerry Brody and Joel Benjamin },
	url={http://www.ibm.com/ibm/history/ibm100/us/en/icons/deepblue/},
	publisher={IBM100}
}

@inproceedings {199317,
	author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
	title = {TensorFlow: A System for Large-Scale Machine Learning},
	booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
	year = {2016},
	isbn = {978-1-931971-33-1},
	address = {Savannah, GA},
	pages = {265--283},
	url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
	publisher = {{USENIX} Association},
	month = nov,
}

@INPROCEEDINGS{8638639,
	author={N. {Dryden} and N. {Maruyama} and T. {Moon} and T. {Benson} and A. {Yoo} and M. {Snir} and B. {Van Essen}},
	booktitle={2018 IEEE/ACM Machine Learning in HPC Environments (MLHPC)},
	title={Aluminum: An Asynchronous, GPU-Aware Communication Library Optimized for Large-Scale Training of Deep Neural Networks on HPC Systems},
	year={2018},
	volume={},
	number={},
	pages={1-13},
	keywords={convolutional neural nets;data analysis;learning (artificial intelligence);neural nets;parallel processing;evolutionary computation;message passing;image classification;gradient methods;stochastic processes;data analysis;convolutional neural nets;neural nets;parallel processing;learning (artificial intelligence);Training;Graphics processing units;Synchronization;Optimization;Benchmark testing;Libraries;Deep learning;machine learning;communication optimization;collective algorithms;HPC},
	doi={10.1109/MLHPC.2018.8638639},
	ISSN={null},
	month={Nov},}

@article{DBLP:journals/corr/abs-1802-05799,
	author    = {Alexander Sergeev and
	Mike Del Balso},
	title     = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
	journal   = {CoRR},
	volume    = {abs/1802.05799},
	year      = {2018},
	url       = {http://arxiv.org/abs/1802.05799},
	archivePrefix = {arXiv},
	eprint    = {1802.05799},
	timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1802-05799.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{8752844,
	author={A. A. {Awan} and J. {Bédorf} and C. {Chu} and H. {Subramoni} and D. K. {Panda}},
	booktitle={2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)},
	title={Scalable Distributed DNN Training using TensorFlow and CUDA-Aware MPI: Characterization, Designs, and Performance Evaluation},
	year={2019},
	volume={},
	number={},
	pages={498-507},
	keywords={application program interfaces;graphics processing units;learning (artificial intelligence);message passing;multiprocessing systems;neural nets;parallel architectures;performance evaluation;scalable distributed DNN training;performance evaluation;Deep Learning;large-scale datasets;efficient CPU;GPU;software frameworks;in-depth performance characterization;distributed TensorFlow;official gRPC-based approaches;Horovod design;Allreduce primitive;efficient CUDA-Aware MPI Allreduce design;CUDA kernels;CUDA-driver query overheads;NCCL2;ResNet-50 training;MPI implementation;Horovod-NCCL;Horovod-NCCL;distributed training capabilities;efficiency 90 percent;TensorFlow;DNN Training;CUDA Aware MPI;Horovod;gRPC;MVAPICH2},
	doi={10.1109/CCGRID.2019.00064},
	ISSN={null},
	month={May},}

@inproceedings{Mendygral2018HighPS,
	title={High Performance Scalable Deep Learning with the Cray Programming Environments Deep Learning Plugin},
	author={Peter Mendygral and Nick Hill and Krishna Kandalla and Diana Moise and Jacob Balma and Marcel Sch{\"o}ngens},
	year={2018}
}

@article{DBLP:journals/corr/abs-1710-11351,
	author    = {Takuya Akiba and
	Keisuke Fukuda and
	Shuji Suzuki},
	title     = {ChainerMN: Scalable Distributed Deep Learning Framework},
	journal   = {CoRR},
	volume    = {abs/1710.11351},
	year      = {2017},
	url       = {http://arxiv.org/abs/1710.11351},
	archivePrefix = {arXiv},
	eprint    = {1710.11351},
	timestamp = {Tue, 17 Sep 2019 14:15:09 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1710-11351.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{7780653,
	author={F. N. {Iandola} and M. W. {Moskewicz} and K. {Ashraf} and K. {Keutzer}},
	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	title={FireCaffe: Near-Linear Acceleration of Deep Neural Network Training on Compute Clusters},
	year={2016},
	volume={},
	number={},
	pages={2592-2600},
	keywords={distributed algorithms;graphics processing units;learning (artificial intelligence);neural net architecture;workstation clusters;FireCaffe;near-linear acceleration;deep neural network training;compute clusters;DNN architectures;GPU cluster;distributed algorithms;network hardware;high bandwidth;GPU servers;Infiniband;Cray interconnects;communication algorithms;reduction trees;batch size;GoogLeNet;Network-in-Network;ImageNet;Training;Computer architecture;Graphics processing units;Servers;Neural networks;Computational modeling;Parallel processing},
	doi={10.1109/CVPR.2016.284},
	ISSN={1063-6919},
	month={June},
}

@inproceedings{10.1145/2834892.2834897,
	author = {Van Essen, Brian and Kim, Hyojin and Pearce, Roger and Boakye, Kofi and Chen, Barry},
	title = {LBANN: Livermore Big Artificial Neural Network HPC Toolkit},
	year = {2015},
	isbn = {9781450340069},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2834892.2834897},
	doi = {10.1145/2834892.2834897},
	booktitle = {Proceedings of the Workshop on Machine Learning in High-Performance Computing Environments},
	articleno = {Article 5},
	numpages = {6},
	keywords = {high performance computing, deep learning, artificial neural networks},
	location = {Austin, Texas},
	series = {MLHPC ’15}
}

@inproceedings{10.1145/3357223.3362707,
	author = {Dai, Jason Jinquan and Wang, Yiheng and Qiu, Xin and Ding, Ding and Zhang, Yao and Wang, Yanzhang and Jia, Xianyan and Zhang, Cherry Li and Wan, Yan and Li, Zhichao and et al.},
	title = {BigDL: A Distributed Deep Learning Framework for Big Data},
	year = {2019},
	isbn = {9781450369732},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3357223.3362707},
	doi = {10.1145/3357223.3362707},
	booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
	pages = {50–60},
	numpages = {11},
	keywords = {Apache Spark, end-to-end data pipeline, big data, distributed deep learning},
	location = {Santa Cruz, CA, USA},
	series = {SoCC ’19}
}

@article{WANG2019615,
	title = "From Intelligence Science to Intelligent Manufacturing",
	journal = "Engineering",
	volume = "5",
	number = "4",
	pages = "615 - 618",
	year = "2019",
	issn = "2095-8099",
	doi = "https://doi.org/10.1016/j.eng.2019.04.011",
	url = "http://www.sciencedirect.com/science/article/pii/S2095809919301821",
	author = "Lihui Wang"
}

@ebook{DLvsML,
	author = "MathWorks",
	title = "Deep Learning vs Machine Learning: Choosing the Best Approach",
	url = "https://explore.mathworks.com/machine-learning-vs-deep-learning",
}

@article{10.1145/3065386,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	year = {2017},
	issue_date = {June 2017},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {60},
	number = {6},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3065386},
	doi = {10.1145/3065386},
	journal = {Commun. ACM},
	month = may,
	pages = {84–90},
	numpages = {7}
}

@ARTICLE{8066291,
	author={C. {Yin} and Y. {Zhu} and J. {Fei} and X. {He}},
	journal={IEEE Access},
	title={A Deep Learning Approach for Intrusion Detection Using Recurrent Neural Networks},
	year={2017},
	volume={5},
	number={},
	pages={21954-21961},}

@InProceedings{Chollet_2017_CVPR,
	author = {Chollet, Francois},
	title = {Xception: Deep Learning With Depthwise Separable Convolutions},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {July},
	year = {2017}
}

@inproceedings{10.5555/3298023.3298188,
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
	title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
	year = {2017},
	publisher = {AAAI Press},
	booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
	pages = {4278\u20134284},
	numpages = {7},
	location = {San Francisco, California, USA},
	series = {AAAI\u201917}
}

@InProceedings{Gidaris_2015_ICCV,
	author = {Gidaris, Spyros and Komodakis, Nikos},
	title = {Object Detection via a Multi-Region and Semantic Segmentation-Aware CNN Model},
	booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
	month = {December},
	year = {2015}
} 

@inproceedings{mikolov2010recurrent,
	title={Recurrent neural network based language model},
	author={Mikolov, Tom{\'a}{\v{s}} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
	booktitle={Eleventh annual conference of the international speech communication association},
	year={2010}
}

@inproceedings{chung2015recurrent,
	title={A recurrent latent variable model for sequential data},
	author={Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
	booktitle={Advances in neural information processing systems},
	pages={2980--2988},
	year={2015}
}

@inproceedings{alahi2016social,
	title={Social lstm: Human trajectory prediction in crowded spaces},
	author={Alahi, Alexandre and Goel, Kratarth and Ramanathan, Vignesh and Robicquet, Alexandre and Fei-Fei, Li and Savarese, Silvio},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={961--971},
	year={2016}
}

@article{DBLP:journals/corr/HuangXY15,
	author    = {Zhiheng Huang and
	Wei Xu and
	Kai Yu},
	title     = {Bidirectional {LSTM-CRF} Models for Sequence Tagging},
	journal   = {CoRR},
	volume    = {abs/1508.01991},
	year      = {2015},
	url       = {http://arxiv.org/abs/1508.01991},
	archivePrefix = {arXiv},
	eprint    = {1508.01991},
	timestamp = {Mon, 13 Aug 2018 16:46:46 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HuangXY15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3146347.3146356,
	author = {Awan, Ammar Ahmad and Subramoni, Hari and Panda, Dhabaleswar K.},
	title = {An In-Depth Performance Characterization of CPU- and GPU-Based DNN Training on Modern Architectures},
	year = {2017},
	isbn = {9781450351379},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3146347.3146356},
	doi = {10.1145/3146347.3146356},
	booktitle = {Proceedings of the Machine Learning on HPC Environments},
	articleno = {8},
	numpages = {8},
	keywords = {Caffe, Deep Learning, High-Performance Computing, Pascal Architecture, Unified Memory},
	location = {Denver, CO, USA},
	series = {MLHPC’17}
}

@misc{Lind1354858,
	author = {Lind, Eric and Pantigoso Velasquez, {\"A}velin},
	institution = {KTH, School of Electrical Engineering and Computer Science (EECS)},
	pages = {28},
	school = {KTH, School of Electrical Engineering and Computer Science (EECS)},
	title = {A performance comparison between CPU and GPU in TensorFlow},
	series = {TRITA-EECS-EX},
	number = {2019:375},
	abstract = {The fast-growing field of Machine Learning has in the later years become more common, as it has gone from a restricted research area to actually be in general use. Frameworks such as TensorFlow have been developed to scale and analyze artificial neural networks, which are used in one of the areas in Machine Learning called Deep Learning. This paper will study how well the framework TensorFlow performs in regard to time and memory allocation on the processor units CPU and GPU since these are the factors that are often the restraining resources. Three neural networks have been used to measure how TensorFlow allocates the resources and computes operations used to process the neural network during the training phase. By using TensorFlows profiler we could trace how each operation was executed in the CPU and GPU, from the gathered data we could analyse how the operations allocated memory and time. Our results show that the training of a more complex neural network benefits from being executed on the GPU, while a simple neural network has no or an insignificant profit from being executed on the GPU over the CPU. The result also indicates possible findings for further research such as processor utilisation as the gaps in the scheduling has not been studied in this paper. },
	year = {2019}
}

@inproceedings{10.1145/3332186.3333152,
	author = {Patel, Trupesh R. and Bodduluri, Sandeep and Anthony, Thomas and Monroe, William S. and Kandhare, Pravinkumar G. and Robinson, John-Paul and Nakhmani, Arie and Zhang, Chengcui and Bhatt, Surya P. and Bangalore, Purushotham V.},
	title = {Performance Characterization of Single and Multi GPU Training of U-Net Architecture for Medical Image Segmentation Tasks},
	year = {2019},
	isbn = {9781450372275},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3332186.3333152},
	doi = {10.1145/3332186.3333152},
	booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (Learning)},
	articleno = {115},
	numpages = {4},
	keywords = {GPU, Medical Image Segmentation, Deep Learning, U-Net},
	location = {Chicago, IL, USA},
	series = {PEARC ’19}
}

@article{10.13140/RG.2.2.22603.54563,
	author = {Kayid, Amr and Khaled, Yasmeen and Elmahdy, Mohamed},
	year = {2018},
	month = {05},
	pages = {},
	title = {Performance of CPUs/GPUs for Deep Learning workloads},
	doi = {10.13140/RG.2.2.22603.54563},
	url = {https://www.researchgate.net/publication/325023664_Performance_of_CPUsGPUs_for_Deep_Learning_workloads},
}

@misc{wood_2019, 
	title={Learn Lisp Programming: Intro, Versions, and More}, 
	url={https://www.whoishostingthis.com/resources/lisp/}, 
	journal={WhoIsHostingThis.com}, 
	author={Wood, Adam Michael}, 
	year={2019}, 
	month={Jul}
}

@inproceedings{NEURIPS2019_bdbca288,
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
	volume = {32},
	year = {2019}
}

@techreport{collobert2002torch,
	title={Torch: a modular machine learning software library},
	author={Collobert, Ronan and Bengio, Samy and Mari{\'e}thoz, Johnny},
	year={2002},
	institution={Idiap}
}

@article{guennebaud2010eigen,
	title={Eigen},
	author={Guennebaud, Ga{\"e}l and Jacob, Benoit and others},
	journal={URl: http://eigen. tuxfamily. org},
	volume={3},
	year={2010}
}

@article{lecun2002technical,
	title={Technical report: Lush reference manual, code available at http://lush. sourceforge. net},
	author={Lecun, Yann and Bottou, Leon},
	year={2002}
}

@misc{neubig2017dynet,
	title={DyNet: The Dynamic Neural Network Toolkit}, 
	author={Graham Neubig and Chris Dyer and Yoav Goldberg and Austin Matthews and Waleed Ammar and Antonios Anastasopoulos and Miguel Ballesteros and David Chiang and Daniel Clothiaux and Trevor Cohn and Kevin Duh and Manaal Faruqui and Cynthia Gan and Dan Garrette and Yangfeng Ji and Lingpeng Kong and Adhiguna Kuncoro and Gaurav Kumar and Chaitanya Malaviya and Paul Michel and Yusuke Oda and Matthew Richardson and Naomi Saphra and Swabha Swayamdipta and Pengcheng Yin},
	year={2017},
	eprint={1701.03980},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

 @misc{jones_oliphant_peterson_2001, 
 	title={Scipy: Open source scientific tools for Python},
 	url={https://scipy.org/}, 
 	author={Jones, Eric and Oliphant, Travis and Peterson, Pearu}, 
 	year={2001}
 } 

@inproceedings{graham2005open,
	title={Open MPI: A flexible high performance MPI},
	author={Graham, Richard L and Woodall, Timothy S and Squyres, Jeffrey M},
	booktitle={International Conference on Parallel Processing and Applied Mathematics},
	pages={228--239},
	year={2005},
	organization={Springer}
}

@inproceedings{barker2015message,
	title={Message passing interface (mpi)},
	author={Barker, Brandon},
	booktitle={Workshop: High Performance Computing on Stampede},
	volume={262},
	year={2015}
}

@inproceedings{gropp2002mpich2,
	title={MPICH2: A new start for MPI implementations},
	author={Gropp, William},
	booktitle={European Parallel Virtual Machine/Message Passing Interface Users’ Group Meeting},
	pages={7--7},
	year={2002},
	organization={Springer}
}

@InProceedings{10.1007/978-3-642-10646-0_3,
	author="Shi, Zhongzhi",
	editor="Sakai, Hiroshi
	and Chakraborty, Mihir Kumar
	and Hassanien, Aboul Ella
	and {\'{S}}l{\k{e}}zak, Dominik
	and Zhu, William",
	title="Intelligent Science",
	booktitle="Rough Sets, Fuzzy Sets, Data Mining and Granular Computing",
	year="2009",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="20--32",
	abstract="Intelligence Science is an interdisciplinary subject which dedicates to joint research on basic theory and technology of intelligence by brain science, cognitive science, artificial intelligence and others. Brain science explores the essence of brain, research on the principles and models of natural intelligence at molecular, cellular, and behavior levels. Cognitive science studies human mental activities, such as perception, learning, memory, thinking, consciousness etc. Artificial intelligence attempts simulation, extension, and expansion of human intelligence using artificial methods and technologies. Researchers specialized in above three disciplines work together to explore new concepts, theories, and methodologies. If successful, it will create a brilliant future in 21st century. The paper will outline the framework of intelligence science and present its ten big challenges. Tolerance Granular Space Model (TGSM) will be discussed as one of helpful approaches.",
	isbn="978-3-642-10646-0"
}

@misc{michael2005artificial,
	title={Artificial intelligence a guide to intelligent systems},
	author={Michael, Negnevitsky},
	year={2005},
	publisher={Addison Wesley}
}

@article{yang2006history,
	title={The History of Artificial Intelligence-AI Winter and its lessons},
	author={Yang, Gary},
	journal={University of Washington (Dec. 2006)},
	year={2006}
}

@article{HistoryofAIforbes,
	title={A Very Short History Of Artificial Intelligence (AI)},
	author={Press, Gil},
	year={2016},
	url={https://www.forbes.com/sites/gilpress/2016/12/30/a-very-short-history-of-artificial-intelligence-ai/?sh=2135e0276fba},
}

@article{HistoryofAIwiki,
	author={Wikipedia},
	title={Timeline of artificial intelligence},
	year={2020},
	url={https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence},
}

@article{HistoryofMLdataversity,
	title={A Brief History of Machine Learning},
	author={Foote, Keith D.},
	year={2019},
	url={https://www.dataversity.net/a-brief-history-of-machine-learning/},
}

@INPROCEEDINGS{5453745,  
	author={S. {Na} and L. {Xumin} and G. {Yong}},  
	booktitle={2010 Third International Symposium on Intelligent Information Technology and Security Informatics},   
	title={Research on k-means Clustering Algorithm: An Improved k-means Clustering Algorithm},
	year={2010},  
	volume={},  
	number={},  
	pages={63-67},  
	doi={10.1109/IITSI.2010.74}
}

@article{10.2307/1403797,
	ISSN = {03067734, 17515823},
	URL = {http://www.jstor.org/stable/1403797},
	author = {Evelyn Fix and J. L. Hodges},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	number = {3},
	pages = {238--247},
	publisher = {[Wiley, International Statistical Institute (ISI)]},
	title = {Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties},
	volume = {57},
	year = {1989}
}

@article{doi:10.1080/00031305.1992.10475879,
	author = { N. S.   Altman },
	title = {An Introduction to Kernel and Nearest-Neighbor Nonparametric Regression},
	journal = {The American Statistician},
	volume = {46},
	number = {3},
	pages = {175-185},
	year  = {1992},
	publisher = {Taylor & Francis},
	doi = {10.1080/00031305.1992.10475879},
	
	URL = { 
	https://www.tandfonline.com/doi/abs/10.1080/00031305.1992.10475879
	
	}
	
}

@article{cortes1995support,
	title={Support-vector networks},
	author={Cortes, Corinna and Vapnik, Vladimir},
	journal={Machine learning},
	volume={20},
	number={3},
	pages={273--297},
	year={1995},
	publisher={Springer}
}

@inproceedings{ho1995random,
	title={Random decision forests},
	author={Ho, Tin Kam},
	booktitle={Proceedings of 3rd international conference on document analysis and recognition},
	volume={1},
	pages={278--282},
	year={1995},
	organization={IEEE}
}

@article{ho1998random,
	author={Ho, Tin Kam},
	title={The random subspace method for constructing decision forests},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={20},
	number={8},
	pages={832--844},
	year={1998},
	publisher={IEEE}
}

@INPROCEEDINGS{477012,  
	author={T. {Starner} and A. {Pentland}},  
	booktitle={Proceedings of International Symposium on Computer Vision - ISCV},   
	title={Real-time American Sign Language recognition from video using hidden Markov models},   
	year={1995},  
	volume={},  
	number={},  
	pages={265-270},  
	doi={10.1109/ISCV.1995.477012}
}

@inproceedings{pardo2005modeling,
	title={Modeling form for on-line following of musical performances},
	author={Pardo, Bryan and Birmingham, William},
	booktitle={Proceedings of the National Conference on Artificial Intelligence},
	volume={20},
	number={2},
	pages={1018},
	year={2005},
	organization={Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999}
}

@inproceedings{chen2016xgboost,
	title={Xgboost: A scalable tree boosting system},
	author={Chen, Tianqi and Guestrin, Carlos},
	booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
	pages={785--794},
	year={2016}
}

@inproceedings{dietterich2000ensemble,
	title={Ensemble methods in machine learning},
	author={Dietterich, Thomas G},
	booktitle={International workshop on multiple classifier systems},
	pages={1--15},
	year={2000},
	organization={Springer}
}

@article{VALUEVA2020232,
	title = "Application of the residue number system to reduce hardware costs of the convolutional neural network implementation",
	journal = "Mathematics and Computers in Simulation",
	volume = "177",
	pages = "232 - 243",
	year = "2020",
	issn = "0378-4754",
	doi = "https://doi.org/10.1016/j.matcom.2020.04.031",
	url = "http://www.sciencedirect.com/science/article/pii/S0378475420301580",
	author = "M.V. Valueva and N.N. Nagornov and P.A. Lyakhov and G.V. Valuev and N.I. Chervyakov",
	keywords = "Image processing, Convolutional neural networks, Residue number system, Quantization noise, Field-programmable gate array (FPGA).",
	abstract = "Convolutional neural networks are a promising tool for solving the problem of pattern recognition. Most well-known convolutional neural networks implementations require a significant amount of memory to store weights in the process of learning and working. We propose a convolutional neural network architecture in which the neural network is divided into hardware and software parts to increase performance and reduce the cost of implementation resources. We also propose to use the residue number system (RNS) in the hardware part to implement the convolutional layer of the neural network. Software simulations using Matlab 2018b showed that convolutional neural network with a minimum number of layers can be quickly and successfully trained. The hardware implementation of the convolution layer shows that the use of RNS allows to reduce the hardware costs on 7.86%–37.78% compared to the two’s complement implementation. The use of the proposed heterogeneous implementation reduces the average time of image recognition by 41.17%."
}

@article{fernandez2009novel,
	title={A novel connectionist system for improved unconstrained handwriting recognition},
	author={Fernandez, AGMLS and Bunke, R Bertolami H and Schmiduber, J},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={31},
	number={5},
	year={2009}
}

@article{hochreiter1997long,
	title={Long short-term memory},
	author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal={Neural computation},
	volume={9},
	number={8},
	pages={1735--1780},
	year={1997},
	publisher={MIT Press}
}

@article{piryonesi2020role,
	title={Role of Data Analytics in Infrastructure Asset Management: Overcoming Data Size and Quality Problems},
	author={Piryonesi, S Madeh and El-Diraby, Tamer E},
	journal={Journal of Transportation Engineering, Part B: Pavements},
	volume={146},
	number={2},
	pages={04020022},
	year={2020},
	publisher={American Society of Civil Engineers}
}

@book{hastie2009elements,
	title={The elements of statistical learning: data mining, inference, and prediction},
	author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year={2009},
	publisher={Springer Science \& Business Media}
}

@article{reynolds2009gaussian,
	title={Gaussian Mixture Models.},
	author={Reynolds, Douglas A},
	journal={Encyclopedia of biometrics},
	volume={741},
	year={2009},
	publisher={Berlin, Springer}
}

@article{goodfellow2014generative,
	title={Generative adversarial nets},
	author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	journal={Advances in neural information processing systems},
	volume={27},
	pages={2672--2680},
	year={2014}
}

@inproceedings{10.1145/3292500.3330756,
	author = {Tokui, Seiya and Okuta, Ryosuke and Akiba, Takuya and Niitani, Yusuke and Ogawa, Toru and Saito, Shunta and Suzuki, Shuji and Uenishi, Kota and Vogel, Brian and Yamazaki Vincent, Hiroyuki},
	title = {Chainer: A Deep Learning Framework for Accelerating the Research Cycle},
	year = {2019},
	isbn = {9781450362016},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3292500.3330756},
	doi = {10.1145/3292500.3330756},
	abstract = {Software frameworks for neural networks play a key role in the development and application
	of deep learning methods. In this paper, we introduce the Chainer framework, which
	intends to provide a flexible, intuitive, and high performance means of implementing
	the full range of deep learning models needed by researchers and practitioners. Chainer
	provides acceleration using Graphics Processing Units with a familiar NumPy-like API
	through CuPy, supports general and dynamic models in Python through Define-by-Run,
	and also provides add-on packages for state-of-the-art computer vision models as well
	as distributed training.},
	booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages = {2002–2011},
	numpages = {10},
	keywords = {gpu computing, distributed training, deep learning frameworks, computer vision},
	location = {Anchorage, AK, USA},
	series = {KDD '19}
}

@article{DBLP:journals/corr/ChenLLLWWXXZZ15,
	author    = {Tianqi Chen and
	Mu Li and
	Yutian Li and
	Min Lin and
	Naiyan Wang and
	Minjie Wang and
	Tianjun Xiao and
	Bing Xu and
	Chiyuan Zhang and
	Zheng Zhang},
	title     = {MXNet: {A} Flexible and Efficient Machine Learning Library for Heterogeneous
	Distributed Systems},
	journal   = {CoRR},
	volume    = {abs/1512.01274},
	year      = {2015},
	url       = {http://arxiv.org/abs/1512.01274},
	archivePrefix = {arXiv},
	eprint    = {1512.01274},
	timestamp = {Mon, 13 Aug 2018 16:48:04 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/ChenLLLWWXXZZ15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{10.1007/978-3-540-30218-6_19,
	author={Gabriel, Edgar
	and Fagg, Graham E.
	and Bosilca, George
	and Angskun, Thara
	and Dongarra, Jack J.
	and Squyres, Jeffrey M.
	and Sahay, Vishal
	and Kambadur, Prabhanjan
	and Barrett, Brian
	and Lumsdaine, Andrew
	and Castain, Ralph H.
	and Daniel, David J.
	and Graham, Richard L.
	and Woodall, Timothy S.},
	editor={Kranzlm{\"u}ller, Dieter
	and Kacsuk, P{\'e}ter
	and Dongarra, Jack},
	title={Open MPI: Goals, Concept, and Design of a Next Generation MPI Implementation},
	booktitle={Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	year={2004},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	pages={97--104},
	abstract={A large number of MPI implementations are currently available, each of which emphasize different aspects of high-performance computing or are intended to solve a specific research problem. The result is a myriad of incompatible MPI implementations, all of which require separate installation, and the combination of which present significant logistical challenges for end users. Building upon prior research, and influenced by experience gained from the code bases of the LAM/MPI, LA-MPI, and FT-MPI projects, Open MPI is an all-new, production-quality MPI-2 implementation that is fundamentally centered around component concepts. Open MPI provides a unique combination of novel features previously unavailable in an open-source, production-quality implementation of MPI. Its component architecture provides both a stable platform for third-party research as well as enabling the run-time composition of independent software add-ons. This paper presents a high-level overview the goals, design, and implementation of Open MPI.},
	isbn={978-3-540-30218-6}
}

@inproceedings{barker2015message,
	title={Message passing interface (mpi)},
	author={Barker, Brandon},
	booktitle={Workshop: High Performance Computing on Stampede},
	volume={262},
	year={2015}
}

@inproceedings{gropp2002mpich2,
	title={MPICH2: A new start for MPI implementations},
	author={Gropp, William},
	booktitle={European Parallel Virtual Machine/Message Passing Interface Users’ Group Meeting},
	pages={7--7},
	year={2002},
	organization={Springer}
}

@INPROCEEDINGS{1630794,
	author={Huang, W. and Santhanaraman, G. and Jin, H.-W. and Gao, Q. and Panda, D.K.},
	booktitle={Sixth IEEE International Symposium on Cluster Computing and the Grid (CCGRID'06)}, 
	title={Design of High Performance MVAPICH2: MPI2 over InfiniBand}, 
	year={2006},
	volume={1},
	number={},
	pages={43-48},
	doi={10.1109/CCGRID.2006.32}
}

@inproceedings{jeaugey2017nccl,
	title={Nccl 2.0},
	author={Jeaugey, Sylvain},
	booktitle={GPU Technology Conference (GTC)},
	journal={GTC},
	year={2017},
	url={https://on-demand.gputechconf.com/gtc/2017/presentation/s7155-jeaugey-nccl.pdf}
}

@misc{chetlur2014cudnn,
	title={cuDNN: Efficient Primitives for Deep Learning}, 
	author={Sharan Chetlur and Cliff Woolley and Philippe Vandermersch and Jonathan Cohen and John Tran and Bryan Catanzaro and Evan Shelhamer},
	year={2014},
	eprint={1410.0759},
	archivePrefix={arXiv},
	primaryClass={cs.NE}
}

@inproceedings{10.1145/2939672.2945397,
	author = {Seide, Frank and Agarwal, Amit},
	title = {CNTK: Microsoft's Open-Source Deep-Learning Toolkit},
	year = {2016},
	isbn = {9781450342322},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2939672.2945397},
	doi = {10.1145/2939672.2945397},
	abstract = {This tutorial will introduce the Computational Network Toolkit, or CNTK, Microsoft's
	cutting-edge open-source deep-learning toolkit for Windows and Linux. CNTK is a powerful
	computation-graph based deep-learning toolkit for training and evaluating deep neural
	networks. Microsoft product groups use CNTK, for example to create the Cortana speech
	models and web ranking. CNTK supports feed-forward, convolutional, and recurrent networks
	for speech, image, and text workloads, also in combination. Popular network types
	are supported either natively (convolution) or can be described as a CNTK configuration
	(LSTM, sequence-to-sequence). CNTK scales to multiple GPU servers and is designed
	around efficiency. The tutorial will give an overview of CNTK's general architecture
	and describe the specific methods and algorithms used for automatic differentiation,
	recurrent-loop inference and execution, memory sharing, on-the-fly randomization of
	large corpora, and multi-server parallelization. We will then show how typical uses
	looks like for relevant tasks like image recognition, sequence-to-sequence modeling,
	and speech recognition.},
	booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {2135},
	numpages = {1},
	keywords = {neural networks, computational networks, CNTK, neural network learning toolkit, deep learning},
	location = {San Francisco, California, USA},
	series = {KDD '16}
}

@misc{github,
	author={GitHub},
	title={GitHub},
	url={https://github.com/},
}

@article{dean2012large,
	title={Large scale distributed deep networks},
	author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
	journal={Advances in neural information processing systems},
	volume={25},
	pages={1223--1231},
	year={2012}
}

@misc{gibson_nicholson_patterson_warrick_black_kokorin_audet_eraly_2016, 
	title={Deeplearning4j: Distributed, open-source deep learning for Java and Scala on Hadoop and Spark},
	url={https://figshare.com/articles/software/deeplearning4j-deeplearning4j-parent-0_4-rc3_8_zip/3362644/2}, 
	DOI={10.6084/m9.figshare.3362644.v2}, 
	abstractNote={Deeplearning4j implements deep artificial neural networks on the JVM, with a Java API and C++ for computation. Deeplearning4j includes recurrent neural networks such as long short-term memory networks, as well as convolutional networks, restricted boltzmann machines, deep-belief networks, deep autoencoders and shallow neural networks such as word2vec. Deeplearning4j integrates with Hadoop and Spark, and is built to train neural networks in parallel on CPUs and GPUs.}, 
	publisher={figshare}, 
	author={Gibson, Adam and Nicholson, Chris and Patterson, Josh and Warrick, Melanie and Black, Alex D. and Kokorin, Vyacheslav and Audet, Samuel and Eraly, Susan}, 
	year={2016}, 
	month={May} 
} 

@Article{Rumelhart1986,
	author={Rumelhart, David E.
	and Hinton, Geoffrey E.
	and Williams, Ronald J.},
	title={Learning representations by back-propagating errors},
	journal={Nature},
	year={1986},
	month={Oct},
	day={01},
	volume={323},
	number={6088},
	pages={533-536},
	abstract={We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	issn={1476-4687},
	doi={10.1038/323533a0},
	url={https://doi.org/10.1038/323533a0}
}

@misc{niu2011hogwild,
	title={HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent}, 
	author={Feng Niu and Benjamin Recht and Christopher Re and Stephen J. Wright},
	year={2011},
	eprint={1106.5730},
	archivePrefix={arXiv},
	primaryClass={math.OC}
}

@article{goodfellow2014generative,
	title={Generative adversarial nets},
	author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	journal={Advances in neural information processing systems},
	volume={27},
	year={2014}
}

@incollection{JORDAN1997471,
	title = {Chapter 25 - Serial Order: A Parallel Distributed Processing Approach},
	editor = {John W. Donahoe and Vivian {Packard Dorsel}},
	series = {Advances in Psychology},
	publisher = {North-Holland},
	volume = {121},
	pages = {471-495},
	year = {1997},
	booktitle = {Neural-Network Models of Cognition},
	issn = {0166-4115},
	doi = {https://doi.org/10.1016/S0166-4115(97)80111-2},
	url = {https://www.sciencedirect.com/science/article/pii/S0166411597801112},
	author = {Michael I. Jordan},
	abstract = {ABSTRACT
	A theory of learned sequential behavior is presented, with a focus on coarticulatory phenomena in speech. The theory is implemented as a recurrent parallel distributed processing network that is trained via a generalized error-correcting algorithm. The basic idea underlying the theory is that both serial order and coarticulatory overlap can be represented in terms of relative levels of activation in a network if a clear distinction is made between the state of the network and the output of the network.}
}

@Article{Mnih2015,
	author={Mnih, Volodymyr
	and Kavukcuoglu, Koray
	and Silver, David
	and Rusu, Andrei A.
	and Veness, Joel
	and Bellemare, Marc G.
	and Graves, Alex
	and Riedmiller, Martin
	and Fidjeland, Andreas K.
	and Ostrovski, Georg
	and Petersen, Stig
	and Beattie, Charles
	and Sadik, Amir
	and Antonoglou, Ioannis
	and King, Helen
	and Kumaran, Dharshan
	and Wierstra, Daan
	and Legg, Shane
	and Hassabis, Demis},
	title={Human-level control through deep reinforcement learning},
	journal={Nature},
	year={2015},
	month={Feb},
	day={01},
	volume={518},
	number={7540},
	pages={529-533},
	abstract={An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	issn={1476-4687},
	doi={10.1038/nature14236},
	url={https://doi.org/10.1038/nature14236}
}

 @misc{intel, 
 	title={Optimize AI applications with Intel® oneapi Deep Neural Network Library}, url={https://www.intel.com/content/www/us/en/developer/tools/oneapi/onednn.html}, 
 	journal={Intel},
 	author={Intel}
  } 

 @misc{google,
 	title={Tensorflow Lite: ML for Mobile and edge devices}, 
 	url={https://www.tensorflow.org/lite/}, 
 	journal={TensorFlow}, 
 	author={Google}
 } 

@inproceedings{collobert2011torch7,
	title={Torch7: A matlab-like environment for machine learning},
	author={Collobert, Ronan and Kavukcuoglu, Koray and Farabet, Cl{\'e}ment},
	booktitle={BigLearn, NIPS workshop},
	number={CONF},
	year={2011}
}

@inproceedings{10.1145/2647868.2654889,
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
	year = {2014},
	isbn = {9781450330633},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2647868.2654889},
	doi = {10.1145/2647868.2654889},
	abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
	booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
	pages = {675–678},
	numpages = {4},
	keywords = {open source, parallel computation, neural networks, computer vision, machine learning},
	location = {Orlando, Florida, USA},
	series = {MM '14}
}

@misc{thetheanodevelopmentteam2016theano,
	title={Theano: A Python framework for fast computation of mathematical expressions}, 
	author={The Theano Development Team and Rami Al-Rfou and Guillaume Alain and Amjad Almahairi and Christof Angermueller and Dzmitry Bahdanau and Nicolas Ballas and Frédéric Bastien and Justin Bayer and Anatoly Belikov and Alexander Belopolsky and Yoshua Bengio and Arnaud Bergeron and James Bergstra and Valentin Bisson and Josh Bleecher Snyder and Nicolas Bouchard and Nicolas Boulanger-Lewandowski and Xavier Bouthillier and Alexandre de Brébisson and Olivier Breuleux and Pierre-Luc Carrier and Kyunghyun Cho and Jan Chorowski and Paul Christiano and Tim Cooijmans and Marc-Alexandre Côté and Myriam Côté and Aaron Courville and Yann N. Dauphin and Olivier Delalleau and Julien Demouth and Guillaume Desjardins and Sander Dieleman and Laurent Dinh and Mélanie Ducoffe and Vincent Dumoulin and Samira Ebrahimi Kahou and Dumitru Erhan and Ziye Fan and Orhan Firat and Mathieu Germain and Xavier Glorot and Ian Goodfellow and Matt Graham and Caglar Gulcehre and Philippe Hamel and Iban Harlouchet and Jean-Philippe Heng and Balázs Hidasi and Sina Honari and Arjun Jain and Sébastien Jean and Kai Jia and Mikhail Korobov and Vivek Kulkarni and Alex Lamb and Pascal Lamblin and Eric Larsen and César Laurent and Sean Lee and Simon Lefrancois and Simon Lemieux and Nicholas Léonard and Zhouhan Lin and Jesse A. Livezey and Cory Lorenz and Jeremiah Lowin and Qianli Ma and Pierre-Antoine Manzagol and Olivier Mastropietro and Robert T. McGibbon and Roland Memisevic and Bart van Merriënboer and Vincent Michalski and Mehdi Mirza and Alberto Orlandi and Christopher Pal and Razvan Pascanu and Mohammad Pezeshki and Colin Raffel and Daniel Renshaw and Matthew Rocklin and Adriana Romero and Markus Roth and Peter Sadowski and John Salvatier and François Savard and Jan Schlüter and John Schulman and Gabriel Schwartz and Iulian Vlad Serban and Dmitriy Serdyuk and Samira Shabanian and Étienne Simon and Sigurd Spieckermann and S. Ramana Subramanyam and Jakub Sygnowski and Jérémie Tanguay and Gijs van Tulder and Joseph Turian and Sebastian Urban and Pascal Vincent and Francesco Visin and Harm de Vries and David Warde-Farley and Dustin J. Webb and Matthew Willson and Kelvin Xu and Lijun Xue and Li Yao and Saizheng Zhang and Ying Zhang},
	year={2016},
	eprint={1605.02688},
	archivePrefix={arXiv},
	primaryClass={cs.SC}
}

 @misc{oliphant_2006, 
 	url={https://numpy.org/}, 
 	journal={NumPy}, 
 	author={Oliphant, Travis}, 
 	year={2006}
 } 

 @misc{gabriel_1991, 
 	title={Lisp: Good news, Bad News, how to win big}, 
 	url={https://dreamsongs.com/WIB.html}, 
 	journal={Worse Is Better}, 
 	author={Gabriel, Richard P.}, 
 	year={1991}
 } 

@inproceedings{wang2014minerva,
	title={Minerva: A scalable and highly efficient training platform for deep learning},
	author={Wang, Minjie and Xiao, Tianjun and Li, Jianpeng and Zhang, Jiaxing and Hong, Chuntao and Zhang, Zheng},
	booktitle={NIPS Workshop, Distributed Machine Learning and Matrix Computations},
	year={2014}
}

@misc{lin2015purine,
	title={Purine: A bi-graph based deep learning framework}, 
	author={Min Lin and Shuo Li and Xuan Luo and Shuicheng Yan},
	year={2015},
	eprint={1412.6249},
	archivePrefix={arXiv},
	primaryClass={cs.NE}
}

@inproceedings {186214,
	author = {Mu Li and David G. Andersen and Jun Woo Park and Alexander J. Smola and Amr Ahmed and Vanja Josifovski and James Long and Eugene J. Shekita and Bor-Yiing Su},
	title = {Scaling Distributed Machine Learning with the Parameter Server},
	booktitle = {11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14)},
	year = {2014},
	isbn = { 978-1-931971-16-4},
	address = {Broomfield, CO},
	pages = {583--598},
	url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li_mu},
	publisher = {USENIX Association},
	month = oct,
}

@article{li2014communication,
	title={Communication efficient distributed machine learning with the parameter server},
	author={Li, Mu and Andersen, David G and Smola, Alexander J and Yu, Kai},
	journal={Advances in Neural Information Processing Systems},
	volume={27},
	pages={19--27},
	year={2014}
}

@techreport{yu2014introduction,
	title={An introduction to computational networks and the computational network toolkit},
	author={Yu, Dong and Eversole, Adam and Seltzer, Mike and Yao, Kaisheng and Huang, Zhiheng and Guenter, Brian and Kuchaiev, Oleksii and Zhang, Yu and Seide, Frank and Wang, Huaming and others},
	year={2014},
	institution={Microsoft Technical Report MSR-TR-2014--112}
}

@misc{li2018variable,
	title={Variable-component deep neural network for robust speech recognition},
	author={Li, Jinyu and Zhao, Rui and Gong, Yifan},
	year={2018},
	month=jul # "~10",
	publisher={Google Patents},
	note={US Patent 10,019,990}
}

@INPROCEEDINGS{5206848,  
	author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},  
	booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},   
	title={ImageNet: A large-scale hierarchical image database},  
	year={2009},  
	volume={}, 
	number={},  
	pages={248-255},  
	doi={10.1109/CVPR.2009.5206848}
}